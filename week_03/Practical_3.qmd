---
title: "Practical_3"
format: html
---

## INFS 2049 Experimental Design

### Joshua Chopin

### 2025-01-10

**(Student: Michael Hudson)**

<hr/>

This week we are going to be looking at completely randomised designs which will be analyzed using ANOVA tests and ad-hoc procedures. The dataset to be used this week relates to sales of books, music and electronics in four major regions; North, South, East and West. The sales data comes in the form of a .csv file. Remember you will need the `readr` package installed to load it using the `read_csv` function. And as always, we should begin by summarising the data.

```{r}
library(tidyverse)
```

```{r}
salesdata <- read_csv("sales.csv")
head(salesdata)
```

```{r}
summary(salesdata)
```

You can see that along with the sales and region data for each transaction we have information about the gender of the customer as well as a variable related to advertising. There are a number of hypotheses we could test using this data, but today we are going to test whether there is a statistically significant difference in average music sales by region. There are four different regions in the dataset (North, South, East and West), so the formulation of the hypothesis test looks like this:

Null Hypothesis ($H_0$): All means are equal i.e.

$H_0: \mu_N = \mu_E = \mu_S = \mu_W$

Alternative Hypothesis ($H_1$): Not all means are equal.

**Q.** Write down some other hypotheses that could be formulated for a one-way ANOVA test using this data.

Previously, we have defaulted to boxplots to get a first look at the data, you are more than welcome to create one on your own, actually -

**Q.** Create a boxplot with region on the x-axis and music sales on the y-axis.

but in this practical we will try a barplot with error bars. Recall from Practical 1 that we can use the aggregate function to get mean values within groups. Here we can modify the aggregate function to get both the mean and standard errors for each region. Note that we calculate standard error using the variance for each region divided by the sample size.

```{r}
mean_and_se <- aggregate(Music_Sales ~ Region,
                         salesdata,
                         function(x) c(mean = mean(x), se = sqrt(var(x)/length(x))))
mean_and_se$Music_Sales[, 1]
```

```{r}
mean_and_se$Music_Sales[, 2]
```

```{r}
ggplot(data = mean_and_se,
       aes(x = Region, y = Music_Sales[, 1])) +
  geom_bar(stat = "identity", fill = "steelblue") +
  geom_errorbar(aes(ymin = Music_Sales[, 1] - Music_Sales[, 2],
                    ymax = Music_Sales[, 1] + Music_Sales[, 2])) +
  ylab("Mean Music Sales ($)")
```

**Q.** Using the barplot, does it appear there are differences in mean sales between some of the regions? If so, which ones? Which differences would you expect to be statistically significant?

Before conducting the hypothesis test we need to check all the necessary requirements. The first of which is whether all the samples are normally distributed. To do so we will use the Shapiro-Wilk test introduced last week.

```{r}
shapiro.test(salesdata$Music_Sales[salesdata$Region == "West"])
```

```{r}
shapiro.test(salesdata$Music_Sales[salesdata$Region == "East"])
```

```{r}
shapiro.test(salesdata$Music_Sales[salesdata$Region == "South"])
```

```{r}
shapiro.test(salesdata$Music_Sales[salesdata$Region == "North"])
```

We can see that for the West, East and South regions, the *p* value is greater than 0.05 so normality can be assumed. The North region has a *p* value of approximately 0.02, however the sample size is reasonably large (>30) and the distribution is not severly skewed so we can proceed.

**Q.** The boxplots created earlier give us some indication of whether the data from the North region is skewed. Try creating a histogram to further support this claim.

The next requirement to check is whether variances among the samples are equal.

```{r}
region_sd <- aggregate(salesdata$Music_Sales, list(salesdata$Region), sd)
region_sd
```

$$
\frac{\sigma_{largest}}{\sigma_{smallest}}=\frac{19.58447}{11.16794}=1.753<2
$$

It looks like the variances of the samples may be equal, as the result is less than two, but it is close. To investigate further, we can use some of R's in-built functions to test for equality of variance, such as Bartlett's or Levene's test.

```{r}
bartlett.test(Music_Sales ~ Region, data = salesdata)
```

Here the p-value is well under the target value of 0.05, suggesting that there may be evidence to suggest that the variance across regions is statistically significantly different. Lets try Levene's test, it requires the `car` package.

```{r}
library(car)
```

```{r}
leveneTest(Music_Sales ~ Region, data = salesdata)
```

Once again the result is below the 0.05 threshold, so it appears that the assumption of homogeneity of variance has been violated. As a result, we report Welch's corrected F-ratio instead of the one given in a standard ANOVA table. In R we call the Welch ANOVA using `oneway.test`. It is worth noting that `oneway.test` has an argument called *var.equal*, which takes a value of TRUE or FALSE, depending on whether the variances in the samples should be treated as equal or not, but this defaults to FALSE so we don't need to specify.

```{r}
w_anova <- oneway.test(Music_Sales ~ Region, data = salesdata)
w_anova
```

The p-value is much smaller than our significance level of 0.05, so we can reject the null hypothesis and conclude that there are differences among the mean values of music sales across regions. For interests sake we can also run a standard ANOVA.

```{r}
s_anova <- aov(Music_Sales ~ Region, data = salesdata)
summary(s_anova)
```

and see that in this case the conclusion would have been the same.

Next, we can look at the parameter estimates (differences in mean) relative to the mean for the East region.

```{r}
summary(lm(Music_Sales ~ Region, data = salesdata))
```

One of the first things to note from the summary table is that all mean differences are statistically significant. However, the summary table also gives us some indication of the ways that they differ.

**Q.** The estimate for (Intercept) represents the mean value in the East region. What do the estimates for the other regions represent? Why are they all negative?

In order to get a better idea of these mean differences we can conduct an ad-hoc test. There are a number of different post-hoc tests that can be conducted following a hypothesis test. One of the most popular options, that we will now use, is Tukey's test, also known as the Tukey method, also known as Tukey's HSD (Honestly Significant Difference). Tukey's test should be carried out on the results of standard ANOVA rather than the Welch's ANOVA that we used previously. We will run Tukey's on the Standard ANOVA we created, then run a different pairwise test for the Welch's ANOVA.

```{r}
tukey_test <- TukeyHSD(s_anova)
tukey_test
```

There are a lot of numbers in the output, but a few things stand out. The adjusted p-value is significant at the 0.05 level for all mean differences except for North and South. Furthermore, the largest difference by far appears to be between the East and West regions. We can visualise these results using base R's `plot` command. By default if we don't give R any information about how the data should be plotted, it will do its best to choose the right format based on the data. You can see below that we can simply pass *tukey.test* to the plot command to get a nice visualisation of the confidence intervals (I had to change a couple of settings to rotate axis labels and keep them on the page).

```{r}
par(mar = c(5, 6, 4, 1) + 0.1)
plot(tukey_test, las = 1)
```

As the numbers indicated, the difference in mean between the South and North region might not be significant. Also keep in mind when interpreting these values that sign is important. West - North has a value of -12.66, so the negative sign tells us that North had the greater mean music sales.

The ad-hoc test we use following Welch's ANOVA is called Games-Howell, and we will need to access it through a new package.

```{r}
# install.packages("rstatix")
library(rstatix)
```

The `rstatix` package has a function called `games_howell_test` which we will call by passing first the dataframe, then the formula for our factors and, if necessary, a confidence level, though we will stick with the default 0.95.

```{r}
games_howell_test(salesdata, Music_Sales ~ Region)
```

We can see that for the most part the results between the two tests are similar. The estimate for mean differences is actually identical, but they differ in confidence intervals and adjusted p-values. The overall results don't change, that is, all of the mean differences are significant except for North and South.

**Q.** Do these results confirm your impressions from the error bar plot?

Finally, lets try some different contrasts. Until now we have been testing the hypothesis that not all means are equal, and the post-hoc tests have given us more clarity around the answer to that question. But let us say the hypothesis itself changes, such as; is the mean for the North region different to the mean for the South region? i.e. $H_0: \mu_S = \mu_N$ or is the mean of the East and North regions together different from the mean of the South and West regions? i.e. $H_0: \frac{1}{2}(\mu_E + \mu_N) = \frac{1}{2}(\mu_S + \mu_W)$. There is an easy way to test these hypotheses using our existing ANOVA model and the `gmodels` package.

```{r}
# install.packages("gmodels")
library(gmodels)
```

First we need to convert the Region variable to a factor, then take a look at Region to find the order of the levels.

```{r}
salesdata$Region <- factor(salesdata$Region)
levels(salesdata$Region)
```

Once we know which order the levels are in we can define some contrasts. The below lines of code may not look straight forward but see if you can piece it all together once we print the variable `L` to the screen.

```{r}
con <- matrix(c(0, -1, 1, 0, -1, -1, 1, 1), 4, 2)
L <- t(con)
rownames(L) <- c(" - North v South", " - E&N v S&W")
L
```

Let us explain what happened here. The matrix that we initialised called `con` houses the contrasts. As we have four levels in our Region factor, the first four entries of `con` are the first contrast. Using the order of levels we saw earlier this is a value of 0 for East, -1 for North, 1 for South and 0 for West, indicating that North is the baseline, South is being tested and East and West aren't being used. The next contrast refers to the second hypothesis that we formulated earlier. Finally, the 4, 2 part, shapes the matrix to have 4 columns and 2 rows. We transpose the matrix using the `t` function, then provide some rownames for the two hypotheses. From the `gmodels` package we call the `fit.contrasts` function, providing it with the ANOVA we made earlier, the factor that we are interested in and our matrix of contrasts.

```{r}
fit.contrast(s_anova, "Region", L)
```

Based on the output

My Notes
